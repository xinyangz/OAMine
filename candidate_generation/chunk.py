"""Read impact score matrices generated by preprocess.py and chunk
product titles into attribute value candidates."""

import argparse
import pickle
import re
import unicodedata
from collections import defaultdict
from pathlib import Path
from string import punctuation
from typing import List, Optional, Set
from urllib.request import urlopen

import numpy as np
from tqdm import tqdm

import utils

stopwords = urlopen("https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c")
stopwords = set([s.decode("utf-8").strip() for s in stopwords])

HARD_CUT_SYMBOLS = {",", "|", "(", ")"}

def find_root(parse):
  # root node's head also == 0, so have to be removed
  for token in parse[1:]:
    if token.head == 0:
      return token.id
  return False


def _run_strip_accents(text):
  """Strips accents from a piece of text."""
  text = unicodedata.normalize("NFD", text)
  output = []
  for char in text:
    cat = unicodedata.category(char)
    if cat == "Mn":
      continue
    output.append(char)
  return "".join(output)


def match_tokenized_to_untokenized(subwords, sentence):
  token_subwords = np.zeros(len(sentence))
  sentence = [_run_strip_accents(x) for x in sentence]
  token_ids, subwords_str, current_token, current_token_normalized = [-1] * len(subwords), "", 0, None
  for i, subword in enumerate(subwords):
    if subword in ["[CLS]", "[SEP]", "[UNK]"]:
      continue

    while current_token_normalized is None:
      current_token_normalized = sentence[current_token].lower()

    if subword.startswith("[UNK]"):
      unk_length = int(subword[6:])
      subwords[i] = subword[:5]
      subwords_str += current_token_normalized[len(subwords_str):len(subwords_str) + unk_length]
    else:
      subwords_str += subword[2:] if subword.startswith("##") else subword
    if not current_token_normalized.startswith(subwords_str):
      return False

    token_ids[i] = current_token
    token_subwords[current_token] += 1
    if current_token_normalized == subwords_str:
      subwords_str = ""
      current_token += 1
      current_token_normalized = None

  assert current_token_normalized is None
  while current_token < len(sentence):
    assert not sentence[current_token]
    current_token += 1
  assert current_token == len(sentence)

  return token_ids


def find_best_cut(scores):
  best_score = np.inf
  best_cut = -1
  for k in range(1, len(scores)):
    sq1 = 2 * k
    sq2 = 2 * (len(scores) - k)
    rec = (len(scores) - k) * k
    left = np.sum(scores[:k, :k]) / sq1
    right = np.sum(scores[k + 1:, k + 1:]) / sq2
    between = np.sum(scores[:k, k + 1:]) + np.sum(scores[k + 1:, :k])
    between /= rec
    cut_score = left + right - between
    if cut_score < best_score:
      best_cut = k
      best_score = cut_score
  return best_cut


def mart(scores, sen):
  assert len(scores) == len(sen)

  if len(scores) == 1:
    parse_tree = sen[0]
  else:
    idx_max = find_best_cut(scores)
    parse_tree = []
    if len(sen[:idx_max]) > 0:
      tree0 = mart(scores[:idx_max, :idx_max], sen[:idx_max])
      parse_tree.append(tree0)
    tree1 = sen[idx_max]
    if len(sen[idx_max + 1:]) > 0:
      tree2 = mart(scores[idx_max + 1:, idx_max + 1:], sen[idx_max + 1:])
      tree1 = [tree1, tree2]
    if parse_tree == []:
      parse_tree = tree1
    else:
      parse_tree.append(tree1)
  return parse_tree


def right_branching(sent):
  if type(sent) is not list:
    return sent
  if len(sent) == 1:
    return sent[0]
  else:
    return [sent[0], right_branching(sent[1:])]


def left_branching(sent):
  if type(sent) is not list:
    return sent
  if len(sent) == 1:
    return sent[0]
  else:
    return [left_branching(sent[:-1]), sent[-1]]


def softmax(x):
  """Compute softmax values for each sets of scores in x."""
  e_x = np.exp(x - np.max(x))
  return e_x / e_x.sum(axis=0)


def decoding(args):
  trees = []
  new_results = []
  with open(args.matrix, 'rb') as f:
    results = pickle.load(f)

  all_asins = []
  for (asin, sen, tokenized_text, init_matrix) in tqdm(results):
    try:
      mapping = match_tokenized_to_untokenized(tokenized_text, sen)
    except:
      continue
    if not mapping:
      continue
    # merge subwords in one row
    merge_column_matrix = []
    for i, line in enumerate(init_matrix):
      new_row = []
      buf = []
      for j in range(0, len(line) - 1):
        buf.append(line[j])
        if mapping[j] != mapping[j + 1]:
          new_row.append(buf[0])
          buf = []
      merge_column_matrix.append(new_row)

    # merge subwords in multi rows
    # transpose the matrix so we can work with row instead of multiple rows
    merge_column_matrix = np.array(merge_column_matrix).transpose()
    merge_column_matrix = merge_column_matrix.tolist()
    final_matrix = []
    for i, line in enumerate(merge_column_matrix):
      new_row = []
      buf = []
      for j in range(0, len(line) - 1):
        buf.append(line[j])
        if mapping[j] != mapping[j + 1]:
          if args.subword == 'max':
            new_row.append(max(buf))
          elif args.subword == 'avg':
            new_row.append((sum(buf) / len(buf)))
          elif args.subword == 'first':
            new_row.append(buf[0])
          buf = []
      final_matrix.append(new_row)

    # transpose to the original matrix
    final_matrix = np.array(final_matrix).transpose()

    # filter some empty matrix (only one word)
    if final_matrix.shape[0] == 0:
      print(final_matrix.shape)
      continue
    assert final_matrix.shape[0] == final_matrix.shape[1]
    final_matrix = final_matrix[1:, 1:]

    final_matrix = softmax(final_matrix)

    np.fill_diagonal(final_matrix, 0.)

    final_matrix = 1. - final_matrix
    np.fill_diagonal(final_matrix, 0.)

    if args.decoder == 'mart':
      parse_tree = mart(final_matrix, sen)
      trees.append(parse_tree)

    if args.decoder == 'right_branching':
      trees.append(right_branching(sen))

    if args.decoder == 'left_branching':
      trees.append(left_branching(sen))

    new_results.append((sen, tokenized_text, init_matrix, final_matrix, trees[-1]))
    all_asins.append(asin)

  # with open(os.path.join(args.output_dir, "new_results.pkl"), "wb") as f:
  #   pickle.dump(new_results, f)

  return all_asins, trees, new_results


######## Code for adjacent score based chunking and post processing
def get_adjacent_scores(final_mat: np.ndarray) -> List[float]:
  adjacent_scores = []
  for i in range(final_mat.shape[0] - 1):
    score = (final_mat[i, i + 1] + final_mat[i + 1, i]) / 2
    adjacent_scores.append(score)
  return adjacent_scores


class AdjThresChunker:
  """Chunk ASIN titles based on word-word impact and a threshold.
  We cut if two adjacent words have a impact score < threshold."""

  cut_symbols: Set[str] = HARD_CUT_SYMBOLS
  stopwords: Set[str] = stopwords

  def __init__(self, thres, num_thres=-1, strip=True):
    """Args:
      - thres: threshold for chunking.
      - num_thres: special threshold for numbers. If -1, num_thres will be set equal to thres.
      - strip: If True, remove leading and trailing stop words for each phrase."""
    self.thres = thres
    if num_thres > 0:
      self.num_thres = num_thres
    else:
      self.num_thres = thres
    self.strip = strip

  def fit_transform(self, docs: List[List[str]], scores: List[List[float]], return_cut_scores=False):
    # length check
    assert len(docs) == len(scores)
    assert len(docs) > 0

    phrase_docs = []
    new_scores = []
    for sent, split_scores in zip(docs, scores):
      phrase_doc, new_split_scores = self._threshold_chunking(sent, split_scores, thres=self.thres, num_thres=self.num_thres)
      phrase_docs.append(phrase_doc)
      new_scores.append(new_split_scores)

    self.cut_scores = new_scores
    if return_cut_scores:
      return phrase_docs, new_scores
    else:
      return phrase_docs

  def _threshold_chunking(self, sent: List[str], scores: List[float], thres: float, num_thres: float):
    """Cut when score is below threshold"""

    cuts = [0]
    new_scores = []  # scores after merge
    for i, score in enumerate(scores):
      # score[i] is dist between sent[i] and sent[i + 1]
      # spetial cases
      # case 1: hard cut
      if self._has_hard_cut_symbol(sent[i], sent[i + 1]):
        cuts.append(i + 1)
        continue
      # case 2: don't cut decimal point numbers
      if self._decimal_number(sent[i], sent[i + 1]):
        continue
      # case 3: number, use smaller threshold
      if self._has_number(sent[i], sent[i + 1]):
        if score < num_thres:
          cuts.append(i + 1)
      # general case
      else:
        if score < thres:
          cuts.append(i + 1)
    cuts.append(len(sent))
    token_chunks = []
    for i in range(len(cuts) - 1):
      chunk = sent[cuts[i]:cuts[i + 1]]
      if i < len(cuts) - 2:  # do not count the EOS cut
        cut_score = scores[cuts[i + 1] - 1]
        new_scores.append(cut_score)
      # handle decimal numbers with manual join
      chunk_join = chunk[0]
      for i in range(1, len(chunk)):
        if self._decimal_number(chunk[i - 1], chunk[i]):
          chunk_join += chunk[i]
        else:
          chunk_join += " " + chunk[i]
      # token_chunks.append(" ".join(sent[cuts[i]:cuts[i + 1]]))
      token_chunks.append(chunk_join)
    if self.strip:
      token_chunks = self._strip_stopwords(token_chunks)
    return token_chunks, new_scores

  def _has_number(self, a: str, b: str) -> bool:
    """Check if either of a pair of strings is number"""
    return a.isdigit() or b.isdigit()

  def _decimal_number(self, a: str, b: str) -> bool:
    """Check if a pair of strings is a fraction of a decimal number.
    E.g., a = '123' b = '.' """
    return (a.isdigit() and b == ".") or (a == "." and b.isdigit())

  def _has_hard_cut_symbol(self, a, b) -> bool:
    return a in self.cut_symbols or b in self.cut_symbols

  def _strip_stopwords(self, chunks: List[str]) -> List[str]:  # This function relies on the global variable stopwords
    processed = []
    for chunk in chunks:
      # strip leading and trailing puncts
      chunk = chunk.strip(punctuation + " \t")
      # remove stop words (single token)
      chunk_len = chunk.count(" ") + 1
      if chunk_len == 1 and chunk in self.stopwords:
        continue
      if chunk:
        processed.append(chunk)
    return processed


class PatternMiningPostprocessor:
  """Pattern mining post processing to merge frequently co-occurring words."""

  def __init__(self, max_gram=3, min_sup=5, conf=0.7, min_cut_score=0.1, replace_numbers=True):
    """Args:
      - max_gram: max length of ngram to consider as pattern.
      - min_sup: minimum support of an ngram to be merged.
      - conf: confidence threshold to merge. = max(ngram_sup / sub_unigram_sup).
      - min_cut_score: do not cut when min(cut_scores) is below this threshold.
      - replace_numbers: whether to replace numbers with [NUM] before pattern mining."""
    self.max_gram = 3
    self.min_sup = 5
    self.conf = 0.7
    self.min_cut_score = 0.1
    self.replace_numbers = replace_numbers
    self.ngram_counts = None
    self.ngram_scores = None

  def fit(self, docs: List[List[str]], scores: Optional[List[List[float]]]=None) -> None:
    """Find frequent n-grams in the corpus"""
    assert self.min_cut_score == 0. or scores is not None
    if self.replace_numbers:
      substitutes = self._substitute_numbers(docs)

    ngram_counts = defaultdict(int)
    ngram_scores = defaultdict(float)
    for i_doc, doc in enumerate(docs):
      for n in range(1, self.max_gram + 1):  # 1...MAX_GRAM
        for i in range(0, len(doc) - n + 1):
          ngram = tuple(doc[i:i + n])
          ngram_counts[ngram] += 1
          if n > 1 and scores is not None:
            split_scores = scores[i_doc]
            ng_scores = split_scores[i:i + n - 1]
            ng_score = min(ng_scores)
            ngram_scores[ngram] = (ngram_scores[ngram] * (ngram_counts[ngram] - 1) + ng_score) / ngram_counts[ngram]
    self.ngram_counts = dict(ngram_counts)
    if scores is not None:
      self.ngram_scores = ngram_scores

    if self.replace_numbers:
      self._fillin_numbers(docs, substitutes)

  def transform(self, docs: List[List[str]]) -> List[List[str]]:
    assert self.min_cut_score == 0 or self.ngram_scores is not None

    def _reduce_segments(segments):

      def _overlap(s1, s2):
        return s2[0] < s1[1]

      def _merge(s1, s2):
        return (min(s1[0], s2[0]), max(s1[1], s2[1]))

      segmented = []
      segments = list(sorted(segments))
      segmented.append(segments[0])
      for segment in segments:
        stack_top = segmented[-1]
        # print("TOP:", stack_top, "SEGMENT", segment)
        if _overlap(stack_top, segment):
          stack_top = segmented.pop()
          merged = _merge(stack_top, segment)
          # print("OVERLAP AND MERGE", segmented, merged)
          segmented.append(merged)
        else:
          segmented.append(segment)
      return segmented

    if self.replace_numbers:
      substitutes = self._substitute_numbers(docs)

    new_docs = []
    for doc in docs:
      if not doc:
        new_docs.append(doc)
        continue
      mergers = []
      # check all ngrams, n >= 2
      # use them to replace single words if
      # frequency conditions are met
      for n in range(2, self.max_gram + 1):  # 2...MAX_GRAM
        for i in range(0, len(doc) - n + 1):
          ngram = tuple(doc[i:i + n])
          ngram_count = self.ngram_counts[ngram]
          ratios = []
          for word in ngram:
            word_count = self.ngram_counts[tuple([word])]
            ratios.append(ngram_count / word_count)
          if ngram_count > self.min_sup and max(ratios) > self.conf:
            if self.ngram_scores is None or (self.ngram_scores[ngram] > self.min_cut_score):
              mergers.append((i, i + n))
      # from all mergers, generate the segments
      for i in range(len(doc)):
        mergers.append((i, i + 1))
      segments = _reduce_segments(mergers)
      new_doc = []
      for segment in segments:
        new_doc.append(" ".join(doc[segment[0]:segment[1]]))
      new_docs.append(new_doc)

    if self.replace_numbers:
      new_docs = self._fillin_numbers(new_docs, substitutes)

    return new_docs

  def fit_transform(self, docs: List[List[str]], scores: Optional[List[List[float]]]=None) -> List[List[str]]:
    self.fit(docs, scores)
    return self.transform(docs)


  def _substitute_numbers(self, docs: List[List[str]]) -> List[List[str]]:
    """Substitute numbers and keep track of all the substitutions."""
    substitutions = []
    number_pattern = re.compile(r"(\d+(?:\.\d+)?)")
    for doc in docs:
      doc_subs = []

      def _sub_and_record(s):
        doc_subs.append(s.group(1))
        return "<NUM>"

      for i, phrase in enumerate(doc):
        doc[i] = re.sub(number_pattern, _sub_and_record, phrase)
      substitutions.append(doc_subs)
    assert len(substitutions) == len(docs)
    return substitutions

  def _fillin_numbers(self, docs: List[List[str]], substitutions: List[List[str]]) -> List[List[str]]:
    """Put numbers back to placed replaced by <NUM>"""
    for doc, doc_sub in zip(docs, substitutions):
      nu = 0
      for i, phrase in enumerate(doc):
        phrase_split = phrase.split("<NUM>")
        phrase_filled = ""
        for j, split in enumerate(phrase_split):
          if j == 0:
            phrase_filled = split
          else:
            phrase_filled += doc_sub[nu] + split
            nu += 1
        doc[i] = phrase_filled
      assert nu == len(doc_sub)
    return docs


def adj_threshold_chunking(args, asins, parsing_output):
  """Chunk sentences according to scores of adjacent tokens."""
  # docs = utils.load_input(INPUT_FILE)
  docs = []
  scores = []
  for output in parsing_output:
    sent, tok, init_mat, final_mat, tree = output
    final_mat = 1 - final_mat  # distance to similarity
    s = get_adjacent_scores(final_mat)
    scores.append(s)
    docs.append(sent)

  chunker = AdjThresChunker(thres=0.45, num_thres=0.2, strip=True)
  phrase_docs, new_scores = chunker.fit_transform(docs, scores, return_cut_scores=True)

  post_processor = PatternMiningPostprocessor()
  phrase_docs = post_processor.fit_transform(phrase_docs, new_scores)
  utils.save_output(Path(args.output_dir, f"{args.output_name}.chunk.jsonl"), phrase_docs)
  utils.TextIO.save(Path(args.output_dir, f"{args.output_name}.asin.txt"), asins)


if __name__ == '__main__':
  parser = argparse.ArgumentParser()

  # Data args
  parser.add_argument('--matrix', default=None, help="Path to preprocessed parsing result pkl file")

  # Decoding args
  parser.add_argument('--decoder', default='mart')
  parser.add_argument('--subword', default='avg')
  parser.add_argument("--output_dir", default=None, help="Path to output")
  parser.add_argument("--output_name", default="coffee", help="The name prefix of output files. Will generate .chunk.jsonl and .asin.txt files.")

  args = parser.parse_args()
  utils.IO.ensure_dir(Path(args.output_dir))
  asins, trees, results = decoding(args)
  adj_threshold_chunking(args, asins, results)
